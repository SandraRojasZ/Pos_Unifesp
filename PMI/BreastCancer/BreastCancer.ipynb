{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d71b0e01-3e55-4a5a-8c93-9cdf6e169226",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d71b0e01-3e55-4a5a-8c93-9cdf6e169226",
        "outputId": "62898279-aeb2-4fbd-83ff-a9864a0d1be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cbis-ddsm-breast-cancer-image-dataset' dataset.\n",
            "Dataset baixado em: /kaggle/input/cbis-ddsm-breast-cancer-image-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"awsaf49/cbis-ddsm-breast-cancer-image-dataset\")\n",
        "\n",
        "print(\"Dataset baixado em:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Monta o Google Drive na máquina virtual do Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verifica se o arquivo está lá (ajuste o caminho se mudou o nome da pasta)\n",
        "zip_path = '/content/drive/MyDrive/Unifesp/Projeto_Cancer/archive.zip'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Arquivo zip encontrado com sucesso!\")\n",
        "else:\n",
        "    print(f\"ERRO: Não encontrei o arquivo em {zip_path}. Verifique o nome da pasta/arquivo no Drive.\")"
      ],
      "metadata": {
        "id": "qy6U0nyH00CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6747b3-a992-4da9-9be0-7f642db91a8e"
      },
      "id": "qy6U0nyH00CN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Arquivo zip encontrado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pywt\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. CONFIGURAÇÃO E EXTRAÇÃO ---\n",
        "# Caminho exato identificado na sua imagem\n",
        "ZIP_PATH = '/content/drive/MyDrive/Unifesp/Projeto_Cancer/archive.zip'\n",
        "EXTRACT_PATH = '/content/dados_extraidos'\n",
        "\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    print(f\"Descompactando {ZIP_PATH}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "            zip_ref.extractall(EXTRACT_PATH)\n",
        "        print(\"Descompactação concluída.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao descompactar: {e}\")\n",
        "        exit() # Para se der erro\n",
        "else:\n",
        "    print(\"Arquivos já descompactados anteriormente.\")\n",
        "\n",
        "# --- 2. MAPEAMENTO E PREPARAÇÃO DO DATASET ---\n",
        "print(\"Localizando e processando metadados (CSVs)...\")\n",
        "\n",
        "# Localiza os CSVs de descrição de massa (Tumores) recursivamente\n",
        "csv_train = glob.glob(os.path.join(EXTRACT_PATH, \"**\", \"mass_case_description_train_set.csv\"), recursive=True)\n",
        "csv_test = glob.glob(os.path.join(EXTRACT_PATH, \"**\", \"mass_case_description_test_set.csv\"), recursive=True)\n",
        "\n",
        "if not csv_train:\n",
        "    raise FileNotFoundError(\"CSV de treino não encontrado dentro do zip.\")\n",
        "\n",
        "# Carrega e unifica\n",
        "df = pd.read_csv(csv_train[0])\n",
        "if csv_test:\n",
        "    df_test = pd.read_csv(csv_test[0])\n",
        "    df = pd.concat([df, df_test], ignore_index=True)\n",
        "\n",
        "# Mapeamento de Classes conforme o Artigo [cite: 222-224]\n",
        "# Normal (Benign without callback), Benign, Malignant\n",
        "def get_article_label(pathology):\n",
        "    if 'MALIGNANT' in pathology: return 'Malignant'\n",
        "    if 'BENIGN_WITHOUT_CALLBACK' in pathology: return 'Normal'\n",
        "    if 'BENIGN' in pathology: return 'Benign'\n",
        "    return None\n",
        "\n",
        "df['label'] = df['pathology'].apply(get_article_label)\n",
        "df = df.dropna(subset=['label'])\n",
        "\n",
        "# Indexação das Imagens Físicas (.jpg)\n",
        "print(\"Indexando imagens físicas no disco...\")\n",
        "image_map = {}\n",
        "for img_path in glob.glob(os.path.join(EXTRACT_PATH, \"**\", \"*.jp*g\"), recursive=True):\n",
        "    image_map[os.path.basename(img_path)] = img_path\n",
        "\n",
        "# Cruzamento CSV <-> Imagem\n",
        "valid_data = []\n",
        "for _, row in df.iterrows():\n",
        "    # O ID base da imagem está na primeira parte do caminho no CSV\n",
        "    base_id = row['image file path'].split('/')[0].strip()\n",
        "\n",
        "    found_path = None\n",
        "    # Prioridade 1: Imagem CROP (Recorte do tumor - ROI)\n",
        "    for fname in image_map:\n",
        "        if base_id in fname and \"CROP\" in fname:\n",
        "            found_path = image_map[fname]\n",
        "            break\n",
        "    # Prioridade 2: Imagem FULL (Mamografia completa) se não houver crop\n",
        "    if not found_path:\n",
        "        for fname in image_map:\n",
        "            if base_id in fname and \"FULL\" in fname:\n",
        "                found_path = image_map[fname]\n",
        "                break\n",
        "\n",
        "    if found_path:\n",
        "        valid_data.append({'filepath': found_path, 'label': row['label']})\n",
        "\n",
        "df_final = pd.DataFrame(valid_data)\n",
        "print(f\"Total de imagens vinculadas prontas para uso: {len(df_final)}\")\n",
        "print(\"Distribuição:\", df_final['label'].value_counts())\n",
        "\n",
        "# Divisão Treino/Teste (70/30) [cite: 258, 324]\n",
        "train_df, val_df = train_test_split(df_final, test_size=0.3, stratify=df_final['label'], random_state=42)\n",
        "\n",
        "# --- 3. PIPELINE DE PRÉ-PROCESSAMENTO (Fig. 1 do Artigo) ---\n",
        "def process_pipeline(image_path):\n",
        "    # Leitura\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    # Redimensionar para entrada da GoogleNet [cite: 152]\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding (Remoção de Fundo/Músculo) [cite: 84-86]\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter (Remoção de Ruído, SNR ~0.2) [cite: 87, 97-105]\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter (Realce de Contraste) [cite: 106-108]\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet Packet Decomposition (db3, nível 2) para Suavização [cite: 109-115]\n",
        "    # Decompõe\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    # Zera os coeficientes de detalhe do nível 1 para suavizar (Denoising)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]])\n",
        "    # Reconstrói\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converter para RGB (GoogleNet exige 3 canais)\n",
        "    return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) / 255.0\n",
        "\n",
        "# Gerador de Dados (Para processar em tempo real sem estourar a RAM)\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=16): # Batch ajustado\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.le = LabelEncoder()\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.n_classes = 3\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "    def __len__(self): return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            X.append(process_pipeline(row['filepath']))\n",
        "            y.append(row['enc'])\n",
        "        return np.array(X), to_categorical(y, num_classes=3)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "train_gen = DDSMGenerator(train_df, batch_size=10) # Batch 10 conforme [cite: 326]\n",
        "val_gen = DDSMGenerator(val_df, batch_size=10)\n",
        "\n",
        "# --- 4. REDE NEURAL E TREINAMENTO (ALTERAÇÃO DE PESOS) ---\n",
        "print(\"\\nInicializando GoogleNet com Adam...\")\n",
        "\n",
        "# GoogleNet (Inception) [cite: 14, 260]\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Camadas Finais Modificadas [cite: 279]\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x) # Dropout 50%\n",
        "x = Dense(1024, activation='relu')(x) # Fully Connected\n",
        "predictions = Dense(3, activation='softmax')(x) # 3 Classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação: Adam, LR=0.0001 (O melhor cenário do artigo) [cite: 15, 536, 618]\n",
        "# Aqui ocorre a definição de como os pesos serão alterados\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Iniciando treinamento por 5 épocas (Demonstração da alteração de pesos)...\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5, # Use 10 ou mais para replicar totalmente, 5 para teste rápido\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Treinamento concluído.\")"
      ],
      "metadata": {
        "id": "AfVc7btJmSiY",
        "outputId": "03b1a1dd-9a3d-41b2-e278-a08a821c9ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "id": "AfVc7btJmSiY",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descompactando /content/drive/MyDrive/Unifesp/Projeto_Cancer/archive.zip...\n",
            "Descompactação concluída.\n",
            "Localizando e processando metadados (CSVs)...\n",
            "Indexando imagens físicas no disco...\n",
            "Total de imagens vinculadas prontas para uso: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-895126367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total de imagens vinculadas prontas para uso: {len(df_final)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribuição:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Divisão Treino/Teste (70/30) [cite: 258, 324]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}