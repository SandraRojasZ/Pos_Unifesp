{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71b0e01-3e55-4a5a-8c93-9cdf6e169226",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d71b0e01-3e55-4a5a-8c93-9cdf6e169226",
        "outputId": "62898279-aeb2-4fbd-83ff-a9864a0d1be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cbis-ddsm-breast-cancer-image-dataset' dataset.\n",
            "Dataset baixado em: /kaggle/input/cbis-ddsm-breast-cancer-image-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download\n",
        "path = kagglehub.dataset_download(\"awsaf49/cbis-ddsm-breast-cancer-image-dataset\")\n",
        "\n",
        "print(\"Dataset baixado em:\", path)\n",
        "\n",
        "#https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte 1**\n",
        "\n",
        "Verifica o caminho da pasta"
      ],
      "metadata": {
        "id": "mA0alUokXosQ"
      },
      "id": "mA0alUokXosQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Monta o Google Drive na máquina virtual do Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Unifesp/Projeto_Cancer/archive.zip'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Arquivo zip encontrado com sucesso!\")\n",
        "else:\n",
        "    print(f\"ERRO: Não encontrei o arquivo em {zip_path}. Verifique o nome da pasta/arquivo no Drive.\")"
      ],
      "metadata": {
        "id": "qy6U0nyH00CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbee3eb5-9faf-4de3-976c-72fe125750f7"
      },
      "id": "qy6U0nyH00CN",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Arquivo zip encontrado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte 1.1**\n",
        "\n",
        "Encontra as imagens e combina com as informações (rótulos) do arquivo de metadados (CSV) para que o programa possa aprender com elas.\n"
      ],
      "metadata": {
        "id": "a2PpJHh3yDx0"
      },
      "id": "a2PpJHh3yDx0"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. CONFIGURAÇÃO E EXTRAÇÃO ---\n",
        "# Caminho da imagem\n",
        "ZIP_PATH = '/content/drive/MyDrive/Unifesp/Projeto_Cancer/archive.zip'\n",
        "EXTRACT_PATH = '/content/dados_extraidos'\n",
        "\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    print(f\"Descompactando {ZIP_PATH}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "            zip_ref.extractall(EXTRACT_PATH)\n",
        "        print(\"Descompactação concluída.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao descompactar: {e}\")\n",
        "        exit() # Para se der erro\n",
        "else:\n",
        "    print(\"Arquivos já descompactados anteriormente.\")\n",
        "\n",
        "# --- 2. MAPEAMENTO E PREPARAÇÃO DO DATASET (MODO POR PASTA/UID) ---\n",
        "print(\"Localizando e processando metadados (CSVs)...\")\n",
        "\n",
        "csv_train = glob.glob(os.path.join(EXTRACT_PATH, \"**\", \"mass_case_description_train_set.csv\"), recursive=True)\n",
        "csv_test = glob.glob(os.path.join(EXTRACT_PATH, \"**\", \"mass_case_description_test_set.csv\"), recursive=True)\n",
        "\n",
        "if not csv_train:\n",
        "    raise FileNotFoundError(\"CSV de treino não encontrado.\")\n",
        "\n",
        "df = pd.read_csv(csv_train[0])\n",
        "if csv_test:\n",
        "    df_test = pd.read_csv(csv_test[0])\n",
        "    df = pd.concat([df, df_test], ignore_index=True)\n",
        "\n",
        "# Mapeamento de Classes\n",
        "def get_article_label(pathology):\n",
        "    if 'MALIGNANT' in pathology: return 'Malignant'\n",
        "    if 'BENIGN_WITHOUT_CALLBACK' in pathology: return 'Normal'\n",
        "    if 'BENIGN' in pathology: return 'Benign'\n",
        "    return None\n",
        "\n",
        "df['label'] = df['pathology'].apply(get_article_label)\n",
        "df = df.dropna(subset=['label'])\n",
        "\n",
        "# --- INDEXA POR NOME DA PASTA (UID) ---\n",
        "print(\"Indexando imagens pelo ID da pasta...\")\n",
        "folder_map = {}\n",
        "\n",
        "# Varre todas as pastas e guarda onde estão as imagens JPG\n",
        "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
        "    # Filtra apenas arquivos de imagem\n",
        "    images_in_folder = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    if images_in_folder:\n",
        "        # O nome da pasta atual (aquele numero 1.3.6.1.4...) será a chave\n",
        "        folder_name = os.path.basename(root)\n",
        "\n",
        "        # Guarda o caminho completo das imagens dessa pasta\n",
        "        full_paths = [os.path.join(root, img) for img in images_in_folder]\n",
        "        folder_map[folder_name] = full_paths\n",
        "\n",
        "print(f\"Total de pastas com imagens encontradas: {len(folder_map)}\")\n",
        "\n",
        "# Cruzamento CSV <-> Imagem\n",
        "valid_data = []\n",
        "print(\"Cruzando dados (Buscando ID da pasta do CSV no disco)...\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Preferência: Imagem Recortada (ROI) > Imagem Completa\n",
        "    # O CSV tem caminhos tipo: \"Mass-Training_.../1.3.6.1.4.1...UID.../000000.dcm\"\n",
        "    paths_to_try = [row.get('cropped image file path'), row.get('image file path')]\n",
        "\n",
        "    found_filepath = None\n",
        "\n",
        "    for csv_path in paths_to_try:\n",
        "        if pd.isna(csv_path): continue\n",
        "\n",
        "        # Quebra o caminho do CSV em partes (separado por /)\n",
        "        # Ex: ['Mass-Training...', '1.3.6.1.4.1.9590...', '1.3.6.1.4.1.9590...', '000000.dcm']\n",
        "        path_parts = csv_path.split('/')\n",
        "\n",
        "        # Verifica se alguma parte desse caminho corresponde a uma pasta que achamos no disco\n",
        "        for part in path_parts:\n",
        "            if part in folder_map:\n",
        "                candidates = folder_map[part]\n",
        "\n",
        "                # SE TIVER MAIS DE UMA IMAGEM NA PASTA (Caso da sua imagem 3: ROI + Máscara)\n",
        "                # Pega a maior imagem (geralmente a textura do ROI é maior que a máscara binária)\n",
        "                if len(candidates) > 1:\n",
        "                    try:\n",
        "                        # Pega o arquivo com maior tamanho em bytes (evita pegar a máscara preta)\n",
        "                        found_filepath = max(candidates, key=os.path.getsize)\n",
        "                    except:\n",
        "                        found_filepath = candidates[0]\n",
        "                else:\n",
        "                    found_filepath = candidates[0]\n",
        "\n",
        "                break\n",
        "\n",
        "        if found_filepath: break\n",
        "\n",
        "    if found_filepath:\n",
        "        valid_data.append({'filepath': found_filepath, 'label': row['label']})\n",
        "\n",
        "df_final = pd.DataFrame(valid_data)\n",
        "\n",
        "# --- VERIFICAÇÃO FINAL ---\n",
        "if len(df_final) == 0:\n",
        "    print(\"\\nERRO CRÍTICO PERSISTE.\")\n",
        "    print(\"Debug - Chaves encontradas no disco (exemplo):\", list(folder_map.keys())[:3])\n",
        "    print(\"Debug - Caminho no CSV (exemplo):\", df.iloc[0]['image file path'])\n",
        "    raise ValueError(\"Não foi possível casar as pastas do CSV com as pastas do disco.\")\n",
        "\n",
        "print(f\"SUCESSO! Total de imagens vinculadas: {len(df_final)}\")\n",
        "print(\"Distribuição:\\n\", df_final['label'].value_counts())"
      ],
      "metadata": {
        "id": "AfVc7btJmSiY",
        "outputId": "ffca3b99-d4df-48cc-cfea-ddb1668a0564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AfVc7btJmSiY",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos já descompactados anteriormente.\n",
            "Localizando e processando metadados (CSVs)...\n",
            "Indexando imagens pelo ID da pasta...\n",
            "Total de pastas com imagens encontradas: 6774\n",
            "Cruzando dados (Buscando ID da pasta do CSV no disco)...\n",
            "SUCESSO! Total de imagens vinculadas: 1696\n",
            "Distribuição:\n",
            " label\n",
            "Malignant    784\n",
            "Benign       771\n",
            "Normal       141\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oiRZFiXhhvJ2",
        "outputId": "758bba7a-2b0c-4db2-9f8a-e2bd3375bf63"
      },
      "id": "oiRZFiXhhvJ2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath      label\n",
              "0  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...  Malignant\n",
              "1  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...  Malignant\n",
              "2  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign\n",
              "3  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign\n",
              "4  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7be1db55-aaa4-469f-9063-50cf06209f9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Malignant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Malignant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7be1db55-aaa4-469f-9063-50cf06209f9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7be1db55-aaa4-469f-9063-50cf06209f9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7be1db55-aaa4-469f-9063-50cf06209f9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-42005f44-af2c-4dc8-8c9c-cd454a912b20\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42005f44-af2c-4dc8-8c9c-cd454a912b20')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-42005f44-af2c-4dc8-8c9c-cd454a912b20 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final",
              "summary": "{\n  \"name\": \"df_final\",\n  \"rows\": 1696,\n  \"fields\": [\n    {\n      \"column\": \"filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1696,\n        \"samples\": [\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.344919126411162871929480697862514268877/2-046.jpg\",\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.325678795912867694418543446014194692233/1-128.jpg\",\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.375750281512821445331731529121143204781/2-078.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Malignant\",\n          \"Benign\",\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte 2**\n",
        "\n",
        "Filtro para termos apenas as bases quase balanceadas devida a mesma quantidade entre dados de maligno e benigno"
      ],
      "metadata": {
        "id": "GRtbo19TktbT"
      },
      "id": "GRtbo19TktbT"
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_binario = df_final[df_final['label'] != 'Normal']\n",
        "\n",
        "# Reseta o índice\n",
        "df_final_binario = df_final_binario.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total de imagens restantes: {len(df_final_binario)}\")\n",
        "print(\"Nova distribuição:\\n\", df_final_binario['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFl6fpb7iUVE",
        "outputId": "37ff5c33-bb41-47cf-aea4-8e5a15ebfc9e"
      },
      "id": "JFl6fpb7iUVE",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de imagens restantes: 1555\n",
            "Nova distribuição:\n",
            " label\n",
            "Malignant    784\n",
            "Benign       771\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_binario.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PPICrDuajYyC",
        "outputId": "f7e689ba-21cf-4781-dbbb-e5fc94b1b4ce"
      },
      "id": "PPICrDuajYyC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath      label\n",
              "0  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...  Malignant\n",
              "1  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...  Malignant\n",
              "2  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign\n",
              "3  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign\n",
              "4  /content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...     Benign"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18ca11b6-7e85-4a3e-8884-ef8e053722de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Malignant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Malignant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590...</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18ca11b6-7e85-4a3e-8884-ef8e053722de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18ca11b6-7e85-4a3e-8884-ef8e053722de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18ca11b6-7e85-4a3e-8884-ef8e053722de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4daca438-34d7-4f80-a870-b09744d0605d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4daca438-34d7-4f80-a870-b09744d0605d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4daca438-34d7-4f80-a870-b09744d0605d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final_binario",
              "summary": "{\n  \"name\": \"df_final_binario\",\n  \"rows\": 1555,\n  \"fields\": [\n    {\n      \"column\": \"filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1555,\n        \"samples\": [\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.332921884313361268429213728330957943425/2-268.jpg\",\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.172660126711340156615068937342838051818/1-148.jpg\",\n          \"/content/dados_extraidos/jpeg/1.3.6.1.4.1.9590.100.1.2.121509624512877844505779746973203818149/2-128.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Benign\",\n          \"Malignant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imagem Original**"
      ],
      "metadata": {
        "id": "4Ep9A_zZhCGo"
      },
      "id": "4Ep9A_zZhCGo"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def visualizar_amostra_leitura(df, num_samples=5):\n",
        "    # Seleciona índices aleatórios\n",
        "    indices = np.random.choice(df.index, num_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        caminho = df.loc[idx, 'filepath']\n",
        "        rotulo = df.loc[idx, 'label']\n",
        "\n",
        "        # Leitura da imagem\n",
        "        img = cv2.imread(caminho)\n",
        "\n",
        "        if img is not None:\n",
        "            # OpenCV carrega em BGR (Azul-Verde-Vermelho) e convertemos para RGB para o Matplotlib\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            altura, largura, canais = img.shape\n",
        "\n",
        "            plt.subplot(1, num_samples, i + 1)\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.title(f\"{rotulo}\\nDim: {largura}x{altura}\")\n",
        "            plt.axis('off')\n",
        "        else:\n",
        "            print(f\"Erro ao ler a imagem no índice {idx}: {caminho}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Chama a função\n",
        "print(\"Visualizando 5 imagens aleatórias dos dados de leitura...\")\n",
        "visualizar_amostra_leitura(df_final_binario)"
      ],
      "metadata": {
        "id": "3bRgsJFYg81f",
        "outputId": "ef8473b7-52c8-4c51-c461-c96b88583dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "id": "3bRgsJFYg81f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizando 5 imagens aleatórias dos dados de leitura...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divisão Treino e Teste**"
      ],
      "metadata": {
        "id": "HCIipy_-Ib_n"
      },
      "id": "HCIipy_-Ib_n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão Treino/Teste\n",
        "train_df, val_df = train_test_split(df_final_binario, test_size=0.3, stratify=df_final_binario['label'], random_state=42)\n",
        "print(f\"Treino: {len(train_df)} | Teste/Validação: {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Dd-YAZh7el",
        "outputId": "bebb8f9c-5b50-4e93-e811-0becb471e640"
      },
      "id": "36Dd-YAZh7el",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treino: 1088 | Teste/Validação: 467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte 3**"
      ],
      "metadata": {
        "id": "E1yB9lxjgtxr"
      },
      "id": "E1yB9lxjgtxr"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pywt\n",
        "import numpy as np\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "# --- 3. PIPELINE DE PRÉ-PROCESSAMENTO E GERADORES ---\n",
        "\n",
        "# Definição das funções de filtro\n",
        "def process_pipeline(image_path):\n",
        "    # Leitura\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    # Redimensiona\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0) # para remover ruídos pequenos\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet Packet Decomposition (db3, nível 2)\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]]) # Suavização\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converte para RGB\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) / 255.0\n",
        "\n",
        "# Classe do Gerador (2 CLASSES)\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=10):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.le = LabelEncoder()\n",
        "\n",
        "        # O LabelEncoder mapeia automaticamente: Benign -> 0, Malignant -> 1\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "\n",
        "        #  2 classes\n",
        "        self.n_classes = 2\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "        # Guardar o mapeamento para conferência\n",
        "        self.class_map = dict(zip(self.le.classes_, self.le.transform(self.le.classes_)))\n",
        "        print(f\"Mapeamento de Classes gerado: {self.class_map}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            processed_img = process_pipeline(row['filepath'])\n",
        "            X.append(processed_img)\n",
        "            y.append(row['enc'])\n",
        "\n",
        "        # um_classes=2 (pq não tem mais Classe Normal)\n",
        "        return np.array(X), to_categorical(y, num_classes=2)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# Instancia os geradores\n",
        "print(\"Criando geradores de dados...\")\n",
        "# Adicionado a Divisão do que é Treino e Validação/Teste\n",
        "print(\"Gerador de Treino -------\")\n",
        "train_gen = DDSMGenerator(train_df, batch_size=10)\n",
        "print(\"Gerador de Validação/Teste -------\")\n",
        "val_gen = DDSMGenerator(val_df, batch_size=10)\n",
        "print(\"Geradores prontos!\")"
      ],
      "metadata": {
        "id": "SMqLMHE7oq8X",
        "outputId": "9fa7d0c2-ffc6-4a55-c778-8b7fc1152efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SMqLMHE7oq8X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criando geradores de dados...\n",
            "Gerador de Treino -------\n",
            "Mapeamento de Classes gerado: {'Benign': np.int64(0), 'Malignant': np.int64(1)}\n",
            "Gerador de Validação/Teste -------\n",
            "Mapeamento de Classes gerado: {'Benign': np.int64(0), 'Malignant': np.int64(1)}\n",
            "Geradores prontos!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "# --- 4. REDE NEURAL E TREINAMENTO (GOOGLE NET + ADAM) ---\n",
        "print(\"\\nInicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x) # usar para Data augmentation(no conjunto de Treinamento e testar com Taxa de aprendizado 1000 e 10000 e com 50 épocas)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Dados binário, por isso 2\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "print(\"Iniciando treinamento...\")\n",
        "\n",
        "# Treinamento\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=30,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Treinamento concluído.\")"
      ],
      "metadata": {
        "id": "CK_JUPLXtZrM",
        "outputId": "8cf9a521-1e3a-43b4-eb60-9c179ad781dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CK_JUPLXtZrM",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inicializando GoogleNet (InceptionV3)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Iniciando treinamento...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m816s\u001b[0m 7s/step - accuracy: 0.5538 - auc: 0.5619 - loss: 0.7430 - val_accuracy: 0.4957 - val_auc: 0.5237 - val_loss: 0.7465\n",
            "Epoch 2/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 7s/step - accuracy: 0.5950 - auc: 0.6294 - loss: 0.6898 - val_accuracy: 0.5609 - val_auc: 0.5666 - val_loss: 0.6857\n",
            "Epoch 3/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 7s/step - accuracy: 0.5888 - auc: 0.5999 - loss: 0.6942 - val_accuracy: 0.5087 - val_auc: 0.5326 - val_loss: 0.7084\n",
            "Epoch 4/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 7s/step - accuracy: 0.6153 - auc: 0.6627 - loss: 0.6500 - val_accuracy: 0.5391 - val_auc: 0.5560 - val_loss: 0.7504\n",
            "Epoch 5/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 7s/step - accuracy: 0.6263 - auc: 0.6670 - loss: 0.6602 - val_accuracy: 0.5348 - val_auc: 0.5645 - val_loss: 0.8761\n",
            "Epoch 6/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 7s/step - accuracy: 0.6522 - auc: 0.7189 - loss: 0.6121 - val_accuracy: 0.4978 - val_auc: 0.5413 - val_loss: 0.7500\n",
            "Epoch 7/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 7s/step - accuracy: 0.6507 - auc: 0.7071 - loss: 0.6353 - val_accuracy: 0.5761 - val_auc: 0.5905 - val_loss: 0.7286\n",
            "Epoch 8/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m753s\u001b[0m 7s/step - accuracy: 0.7077 - auc: 0.7859 - loss: 0.5521 - val_accuracy: 0.5130 - val_auc: 0.5658 - val_loss: 0.7773\n",
            "Epoch 9/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 7s/step - accuracy: 0.6827 - auc: 0.7698 - loss: 0.5732 - val_accuracy: 0.5522 - val_auc: 0.5763 - val_loss: 1.0329\n",
            "Epoch 10/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 7s/step - accuracy: 0.7679 - auc: 0.8326 - loss: 0.5096 - val_accuracy: 0.5543 - val_auc: 0.5731 - val_loss: 0.7403\n",
            "Epoch 11/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m728s\u001b[0m 7s/step - accuracy: 0.7517 - auc: 0.8105 - loss: 0.5384 - val_accuracy: 0.5500 - val_auc: 0.5588 - val_loss: 1.1263\n",
            "Epoch 12/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 7s/step - accuracy: 0.8141 - auc: 0.8799 - loss: 0.4378 - val_accuracy: 0.5478 - val_auc: 0.6036 - val_loss: 0.7886\n",
            "Epoch 13/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 7s/step - accuracy: 0.8169 - auc: 0.8995 - loss: 0.4072 - val_accuracy: 0.5565 - val_auc: 0.5908 - val_loss: 0.8191\n",
            "Epoch 14/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 7s/step - accuracy: 0.8382 - auc: 0.9146 - loss: 0.3748 - val_accuracy: 0.5630 - val_auc: 0.5876 - val_loss: 0.7409\n",
            "Epoch 15/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 7s/step - accuracy: 0.8584 - auc: 0.9282 - loss: 0.3486 - val_accuracy: 0.5391 - val_auc: 0.5553 - val_loss: 1.4086\n",
            "Epoch 16/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 7s/step - accuracy: 0.8469 - auc: 0.9280 - loss: 0.3466 - val_accuracy: 0.5478 - val_auc: 0.5554 - val_loss: 1.2709\n",
            "Epoch 17/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 6s/step - accuracy: 0.8615 - auc: 0.9437 - loss: 0.3049 - val_accuracy: 0.5391 - val_auc: 0.5498 - val_loss: 0.9690\n",
            "Epoch 18/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 6s/step - accuracy: 0.8905 - auc: 0.9540 - loss: 0.2784 - val_accuracy: 0.5174 - val_auc: 0.5261 - val_loss: 1.0537\n",
            "Epoch 19/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.8790 - auc: 0.9463 - loss: 0.2963 - val_accuracy: 0.5130 - val_auc: 0.5181 - val_loss: 2.9302\n",
            "Epoch 20/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step - accuracy: 0.9196 - auc: 0.9683 - loss: 0.2215 - val_accuracy: 0.5478 - val_auc: 0.5713 - val_loss: 1.9504\n",
            "Epoch 21/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 6s/step - accuracy: 0.9210 - auc: 0.9677 - loss: 0.2256 - val_accuracy: 0.5283 - val_auc: 0.5470 - val_loss: 1.8539\n",
            "Epoch 22/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step - accuracy: 0.9142 - auc: 0.9679 - loss: 0.2266 - val_accuracy: 0.6022 - val_auc: 0.6031 - val_loss: 1.2335\n",
            "Epoch 23/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.9167 - auc: 0.9755 - loss: 0.2024 - val_accuracy: 0.5674 - val_auc: 0.5785 - val_loss: 1.6200\n",
            "Epoch 24/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 6s/step - accuracy: 0.9455 - auc: 0.9857 - loss: 0.1524 - val_accuracy: 0.5326 - val_auc: 0.5548 - val_loss: 1.8089\n",
            "Epoch 25/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 6s/step - accuracy: 0.9402 - auc: 0.9848 - loss: 0.1599 - val_accuracy: 0.5543 - val_auc: 0.5578 - val_loss: 1.7359\n",
            "Epoch 26/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 6s/step - accuracy: 0.9534 - auc: 0.9903 - loss: 0.1193 - val_accuracy: 0.5543 - val_auc: 0.5648 - val_loss: 1.4936\n",
            "Epoch 27/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 6s/step - accuracy: 0.9193 - auc: 0.9707 - loss: 0.2143 - val_accuracy: 0.5739 - val_auc: 0.5911 - val_loss: 1.0504\n",
            "Epoch 28/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 6s/step - accuracy: 0.9549 - auc: 0.9890 - loss: 0.1314 - val_accuracy: 0.5761 - val_auc: 0.6175 - val_loss: 1.5485\n",
            "Epoch 29/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 6s/step - accuracy: 0.9456 - auc: 0.9849 - loss: 0.1553 - val_accuracy: 0.5609 - val_auc: 0.5915 - val_loss: 1.5070\n",
            "Epoch 30/30\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 6s/step - accuracy: 0.9733 - auc: 0.9955 - loss: 0.0896 - val_accuracy: 0.5522 - val_auc: 0.5789 - val_loss: 1.6029\n",
            "Treinamento concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte com época 50, Aumento de Dados de Treino e Taxa de Treinamento 0.0001**"
      ],
      "metadata": {
        "id": "Wyqjp9aYg97s"
      },
      "id": "Wyqjp9aYg97s"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pywt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --- 1. FUNÇÕES DE PRÉ-PROCESSAMENTO  ---\n",
        "# Remoção de fundo -> Wiener Filter -> CLAHE -> Wavelet db3\n",
        "\n",
        "def process_pipeline(image_path):\n",
        "    # Leitura\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    # Redimensiona\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding (Remoção de Fundo)\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter (Remoção de Ruído)\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter (Contraste)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet Packet Decomposition (db3, nível 2)\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]]) # Suavização\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converte para RGB\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) / 255.0\n",
        "\n",
        "# --- 2. DATA AUGMENTATION ---\n",
        "def apply_augmentation(img):\n",
        "    rows, cols, _ = img.shape\n",
        "    if np.random.rand() > 0.5: # Metade das imagens de treino serão espelhadas horizontalmente e a outra metade ficará normal. Isso cria variedade sem distorcer todo o dataset.\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    # Rotação\n",
        "    angle = np.random.uniform(-20, 20)\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, (cols, rows))\n",
        "    return img\n",
        "\n",
        "# --- 3. CLASSE DO GERADOR ---\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=4, augment=False):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.le = LabelEncoder()\n",
        "        # Garante que 'label' existe e mapeia\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size] # [0:4], [4:8]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            processed_img = process_pipeline(row['filepath']) # chama função 1\n",
        "\n",
        "            if self.augment:\n",
        "                processed_img = apply_augmentation(processed_img) # chama função 2\n",
        "\n",
        "            X.append(processed_img)\n",
        "            y.append(row['enc'])\n",
        "\n",
        "        return np.array(X), to_categorical(y, num_classes=2)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# --- 4. MODELO E TREINAMENTO ---\n",
        "print(\"Inicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "# Define InceptionV3\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congela base\n",
        "print(\"Congelando camadas da base...\")\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Cabeça da rede\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # No artigo se usou camada de dropout com 50% de probabilidade para a GoogleNet. Essa camada \"desliga\" aleatoriamente 50% dos neurônios daquela camada a cada passo.\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compila\n",
        "print(\"Compilando o modelo...\")\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "# Instancia Geradores\n",
        "print(\"Configurando geradores...\")\n",
        "train_gen = DDSMGenerator(train_df, batch_size=4, augment=True) # Chama função 3\n",
        "val_gen   = DDSMGenerator(val_df, batch_size=4, augment=False)\n",
        "\n",
        "# Treina\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint('melhor_modelo_google_net.keras', save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        ")\n",
        "print(\"Treinamento finalizado.\")"
      ],
      "metadata": {
        "id": "ydfl2F8PYuhr",
        "outputId": "94346203-65a0-45ef-91c2-24b390acda62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ydfl2F8PYuhr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializando GoogleNet (InceptionV3)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Congelando camadas da base...\n",
            "Compilando o modelo...\n",
            "Configurando geradores...\n",
            "\n",
            "Iniciando treinamento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 1s/step - accuracy: 0.5338 - auc: 0.5247 - loss: 0.8424 - val_accuracy: 0.5409 - val_auc: 0.5863 - val_loss: 0.7937\n",
            "Epoch 2/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 941ms/step - accuracy: 0.5674 - auc: 0.5821 - loss: 0.7963 - val_accuracy: 0.5216 - val_auc: 0.5518 - val_loss: 0.7332\n",
            "Epoch 3/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 973ms/step - accuracy: 0.5561 - auc: 0.5874 - loss: 0.7418 - val_accuracy: 0.5517 - val_auc: 0.5860 - val_loss: 0.6926\n",
            "Epoch 4/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 932ms/step - accuracy: 0.5755 - auc: 0.6122 - loss: 0.7078 - val_accuracy: 0.5841 - val_auc: 0.5892 - val_loss: 0.6933\n",
            "Epoch 5/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 948ms/step - accuracy: 0.5917 - auc: 0.6220 - loss: 0.6818 - val_accuracy: 0.5625 - val_auc: 0.6046 - val_loss: 0.6715\n",
            "Epoch 6/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 947ms/step - accuracy: 0.5752 - auc: 0.6205 - loss: 0.6858 - val_accuracy: 0.5647 - val_auc: 0.6115 - val_loss: 0.6668\n",
            "Epoch 7/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 976ms/step - accuracy: 0.5560 - auc: 0.5716 - loss: 0.7017 - val_accuracy: 0.5841 - val_auc: 0.6148 - val_loss: 0.6708\n",
            "Epoch 8/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 958ms/step - accuracy: 0.6024 - auc: 0.6460 - loss: 0.6597 - val_accuracy: 0.5517 - val_auc: 0.5822 - val_loss: 0.6866\n",
            "Epoch 9/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 928ms/step - accuracy: 0.5615 - auc: 0.6214 - loss: 0.6719 - val_accuracy: 0.5884 - val_auc: 0.6143 - val_loss: 0.6808\n",
            "Epoch 10/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 966ms/step - accuracy: 0.5963 - auc: 0.6287 - loss: 0.6761 - val_accuracy: 0.5409 - val_auc: 0.5781 - val_loss: 0.7173\n",
            "Epoch 11/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 935ms/step - accuracy: 0.5974 - auc: 0.6391 - loss: 0.6642 - val_accuracy: 0.5797 - val_auc: 0.6270 - val_loss: 0.6582\n",
            "Epoch 12/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 937ms/step - accuracy: 0.5766 - auc: 0.6211 - loss: 0.6709 - val_accuracy: 0.5841 - val_auc: 0.6161 - val_loss: 0.6655\n",
            "Epoch 13/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 938ms/step - accuracy: 0.6134 - auc: 0.6502 - loss: 0.6555 - val_accuracy: 0.6013 - val_auc: 0.6196 - val_loss: 0.6667\n",
            "Epoch 14/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 928ms/step - accuracy: 0.5961 - auc: 0.6600 - loss: 0.6520 - val_accuracy: 0.5668 - val_auc: 0.5993 - val_loss: 0.6760\n",
            "Epoch 15/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 940ms/step - accuracy: 0.6393 - auc: 0.6670 - loss: 0.6495 - val_accuracy: 0.5754 - val_auc: 0.6152 - val_loss: 0.6648\n",
            "Epoch 16/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 930ms/step - accuracy: 0.6016 - auc: 0.6419 - loss: 0.6561 - val_accuracy: 0.5862 - val_auc: 0.6133 - val_loss: 0.6657\n",
            "Epoch 17/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 923ms/step - accuracy: 0.5956 - auc: 0.6207 - loss: 0.6672 - val_accuracy: 0.5647 - val_auc: 0.5925 - val_loss: 0.6772\n",
            "Epoch 18/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 933ms/step - accuracy: 0.5824 - auc: 0.6430 - loss: 0.6532 - val_accuracy: 0.5302 - val_auc: 0.5697 - val_loss: 0.6891\n",
            "Epoch 19/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 1s/step - accuracy: 0.5992 - auc: 0.6469 - loss: 0.6513 - val_accuracy: 0.5862 - val_auc: 0.6087 - val_loss: 0.6691\n",
            "Epoch 20/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 937ms/step - accuracy: 0.6265 - auc: 0.6658 - loss: 0.6520 - val_accuracy: 0.5690 - val_auc: 0.5894 - val_loss: 0.6815\n",
            "Epoch 21/50\n",
            "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 925ms/step - accuracy: 0.5728 - auc: 0.6325 - loss: 0.6583 - val_accuracy: 0.5991 - val_auc: 0.6118 - val_loss: 0.6691\n",
            "Treinamento finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pywt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --- 1. FUNÇÕES DE PRÉ-PROCESSAMENTO  ---\n",
        "# Remoção de fundo -> Wiener Filter -> CLAHE -> Wavelet db3\n",
        "\n",
        "def process_pipeline(image_path):\n",
        "    # Leitura\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    # Redimensiona\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding (Remoção de Fundo)\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter (Remoção de Ruído)\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter (Contraste)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet Packet Decomposition (db3, nível 2)\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]]) # Suavização\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converte para RGB\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) / 255.0\n",
        "\n",
        "# --- 2. DATA AUGMENTATION ---\n",
        "def apply_augmentation(img):\n",
        "    rows, cols, _ = img.shape\n",
        "    if np.random.rand() > 0.5: # Flip\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    # Rotação\n",
        "    angle = np.random.uniform(-20, 20)\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, (cols, rows))\n",
        "    return img\n",
        "\n",
        "# --- 3. CLASSE DO GERADOR ---\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=4, augment=False):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.le = LabelEncoder()\n",
        "        # Garante que 'label' existe e mapeia\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            processed_img = process_pipeline(row['filepath']) # chama função de pre processamento\n",
        "\n",
        "            if self.augment:\n",
        "                processed_img = apply_augmentation(processed_img) # chama função de aumento de dados em treino\n",
        "\n",
        "            X.append(processed_img)\n",
        "            y.append(row['enc'])\n",
        "\n",
        "        return np.array(X), to_categorical(y, num_classes=2)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# --- 4. MODELO E TREINAMENTO ---\n",
        "print(\"Inicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "# Define InceptionV3\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congela base\n",
        "print(\"Congelando camadas da base...\")\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Cabeça da rede\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compila\n",
        "print(\"Compilando o modelo...\")\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "# Instancia Geradores\n",
        "print(\"Configurando geradores...\")\n",
        "train_gen = DDSMGenerator(train_df, batch_size=4, augment=True)\n",
        "val_gen   = DDSMGenerator(val_df, batch_size=4, augment=False)\n",
        "\n",
        "# Treina\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint('melhor_modelo_google_net.keras', save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        ")\n",
        "print(\"Treinamento finalizado.\")"
      ],
      "metadata": {
        "id": "gZoLOOer7F2h"
      },
      "id": "gZoLOOer7F2h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com Fine-Tuning\n",
        "\n",
        "Normalização"
      ],
      "metadata": {
        "id": "y2AEnfS0AYKl"
      },
      "id": "y2AEnfS0AYKl"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pywt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --- 1. FUNÇÕES DE PRÉ-PROCESSAMENTO (CORRIGIDA) ---\n",
        "def process_pipeline(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]])\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converte para RGB e redimensiona\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # --- NORMALIZAÇÃO ---\n",
        "    # InceptionV3 espera input entre -1 e 1, não 0 e 1.\n",
        "    # Fórmula: (Imagem / 127.5) - 1.0\n",
        "    return (img_rgb.astype(np.float32) / 127.5) - 1.0\n",
        "\n",
        "# --- 2. DATA AUGMENTATION ---\n",
        "def apply_augmentation(img):\n",
        "    rows, cols, _ = img.shape\n",
        "    if np.random.rand() > 0.5:\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    # Rotação leve\n",
        "    angle = np.random.uniform(-15, 15) # Reduzi um pouco para não perder bordas\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n",
        "    return img\n",
        "\n",
        "# --- 3. CLASSE DO GERADOR (Mantida igual, apenas garantindo batch size) ---\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=8, augment=False):  # tambanho lote\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.le = LabelEncoder()\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            processed_img = process_pipeline(row['filepath']) # chama função 1\n",
        "            if self.augment:\n",
        "                processed_img = apply_augmentation(processed_img) # chama função 2\n",
        "            X.append(processed_img)\n",
        "            y.append(row['enc'])\n",
        "        return np.array(X), to_categorical(y, num_classes=2)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# --- 4. MODELO E TREINAMENTO (COM FINE-TUNING) ---\n",
        "print(\"Inicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# --- ESTRATÉGIA DE FINE-TUNING ---\n",
        "# 1. Congela as primeiras camadas (que detectam bordas simples)\n",
        "# 2. Descongela as ÚLTIMAS camadas (que detectam formas complexas)\n",
        "# O InceptionV3 tem 311 camadas. Se treina as últimas 60.\n",
        "print(\"Configurando Fine-Tuning (Descongelando topo da rede)...\")\n",
        "\n",
        "for layer in base_model.layers[:249]: # Congela até a camada 249\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[249:]: # Treina da 249 até o fim\n",
        "    layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação\n",
        "print(\"Compilando o modelo...\")\n",
        "# Tamanho do Lote\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "print(f\"Configurando geradores (Batch Size: {BATCH_SIZE})...\")\n",
        "train_gen = DDSMGenerator(train_df, batch_size=BATCH_SIZE, augment=True) # chama função 3\n",
        "val_gen   = DDSMGenerator(val_df, batch_size=BATCH_SIZE, augment=False)\n",
        "\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        # Aumentei a paciência para 12 para dar mais tempo de adaptação ao fine-tuning\n",
        "        EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n",
        "        ModelCheckpoint('melhor_modelo_google_net.keras', save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        ")\n",
        "print(\"Treinamento finalizado.\")"
      ],
      "metadata": {
        "id": "ZfRqO3Ox_mh_",
        "outputId": "2e6d3a7e-e3ea-4a66-f019-60bbfc19568b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZfRqO3Ox_mh_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializando GoogleNet (InceptionV3)...\n",
            "Configurando Fine-Tuning (Descongelando topo da rede)...\n",
            "Compilando o modelo...\n",
            "Configurando geradores (Batch Size: 8)...\n",
            "\n",
            "Iniciando treinamento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 2s/step - accuracy: 0.5336 - auc: 0.5518 - loss: 0.7950 - val_accuracy: 0.5323 - val_auc: 0.5584 - val_loss: 0.7293\n",
            "Epoch 2/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - accuracy: 0.5695 - auc: 0.5737 - loss: 0.7201 - val_accuracy: 0.5733 - val_auc: 0.6108 - val_loss: 0.7010\n",
            "Epoch 3/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 2s/step - accuracy: 0.6004 - auc: 0.6415 - loss: 0.6683 - val_accuracy: 0.5776 - val_auc: 0.6251 - val_loss: 0.6757\n",
            "Epoch 4/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 1s/step - accuracy: 0.5653 - auc: 0.6028 - loss: 0.6849 - val_accuracy: 0.5603 - val_auc: 0.5967 - val_loss: 0.6836\n",
            "Epoch 5/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.5548 - auc: 0.5884 - loss: 0.6895 - val_accuracy: 0.5474 - val_auc: 0.5889 - val_loss: 0.7261\n",
            "Epoch 6/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - accuracy: 0.5679 - auc: 0.6095 - loss: 0.6800 - val_accuracy: 0.5582 - val_auc: 0.5848 - val_loss: 0.6999\n",
            "Epoch 7/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1s/step - accuracy: 0.6042 - auc: 0.6616 - loss: 0.6496 - val_accuracy: 0.5668 - val_auc: 0.5905 - val_loss: 0.6877\n",
            "Epoch 8/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - accuracy: 0.5892 - auc: 0.6238 - loss: 0.6693 - val_accuracy: 0.5625 - val_auc: 0.5899 - val_loss: 0.7025\n",
            "Epoch 9/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - accuracy: 0.6019 - auc: 0.6432 - loss: 0.6605 - val_accuracy: 0.5690 - val_auc: 0.6091 - val_loss: 0.6830\n",
            "Epoch 10/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.6155 - auc: 0.6499 - loss: 0.6566 - val_accuracy: 0.5711 - val_auc: 0.5960 - val_loss: 0.7027\n",
            "Epoch 11/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - accuracy: 0.6438 - auc: 0.7004 - loss: 0.6365 - val_accuracy: 0.5733 - val_auc: 0.6049 - val_loss: 0.6972\n",
            "Epoch 12/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.5887 - auc: 0.6441 - loss: 0.6525 - val_accuracy: 0.5496 - val_auc: 0.5651 - val_loss: 0.7408\n",
            "Epoch 13/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.6171 - auc: 0.6820 - loss: 0.6282 - val_accuracy: 0.5409 - val_auc: 0.5710 - val_loss: 0.7164\n",
            "Epoch 14/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.6116 - auc: 0.6620 - loss: 0.6390 - val_accuracy: 0.5409 - val_auc: 0.5727 - val_loss: 0.7502\n",
            "Epoch 15/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - accuracy: 0.6076 - auc: 0.6719 - loss: 0.6331 - val_accuracy: 0.5603 - val_auc: 0.6000 - val_loss: 0.7278\n",
            "Treinamento finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pywt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.restoration import wiener\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --- 1. FUNÇÕES DE PRÉ-PROCESSAMENTO (CORRIGIDA) ---\n",
        "def process_pipeline(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return np.zeros((224,224,3))\n",
        "\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # A) Otsu Thresholding\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    img = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    # B) Wiener Filter\n",
        "    img = img.astype(np.float64) / 255.0\n",
        "    psf = np.ones((5, 5)) / 25\n",
        "    img = wiener(img, psf, balance=0.2)\n",
        "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # C) CLAHE Filter\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # D) Wavelet\n",
        "    coeffs = pywt.wavedec2(img, 'db3', level=2)\n",
        "    coeffs[1] = tuple([np.zeros_like(v) for v in coeffs[1]])\n",
        "    img = pywt.waverec2(coeffs, 'db3')\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Converte para RGB e redimensiona\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # --- NORMALIZAÇÃO ---\n",
        "    # InceptionV3 espera input entre -1 e 1, não 0 e 1.\n",
        "    # Fórmula: (Imagem / 127.5) - 1.0\n",
        "    return (img_rgb.astype(np.float32) / 127.5) - 1.0\n",
        "\n",
        "# --- 2. DATA AUGMENTATION ---\n",
        "def apply_augmentation(img):\n",
        "    rows, cols, _ = img.shape\n",
        "    if np.random.rand() > 0.5:\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    # Rotação leve\n",
        "    angle = np.random.uniform(-15, 15) # Reduzi um pouco para não perder bordas\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n",
        "    return img\n",
        "\n",
        "# --- 3. CLASSE DO GERADOR (Mantida igual, apenas garantindo batch size) ---\n",
        "class DDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=8, augment=False):  # tambanho lote\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.le = LabelEncoder()\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            processed_img = process_pipeline(row['filepath']) # chama função 1\n",
        "            if self.augment:\n",
        "                processed_img = apply_augmentation(processed_img) # chama função 2\n",
        "            X.append(processed_img)\n",
        "            y.append(row['enc'])\n",
        "        return np.array(X), to_categorical(y, num_classes=2)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# --- 4. MODELO E TREINAMENTO (COM FINE-TUNING) ---\n",
        "print(\"Inicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# --- ESTRATÉGIA DE FINE-TUNING ---\n",
        "# 1. Congela as primeiras camadas (que detectam bordas simples)\n",
        "# 2. Descongela as ÚLTIMAS camadas (que detectam formas complexas)\n",
        "# O InceptionV3 tem 311 camadas. Se treina as últimas 60.\n",
        "print(\"Configurando Fine-Tuning (Descongelando topo da rede)...\")\n",
        "\n",
        "for layer in base_model.layers[:249]: # Congela até a camada 249\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[249:]: # Treina da 249 até o fim\n",
        "    layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação\n",
        "print(\"Compilando o modelo...\")\n",
        "# Tamanho do Lote\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "print(f\"Configurando geradores (Batch Size: {BATCH_SIZE})...\")\n",
        "train_gen = DDSMGenerator(train_df, batch_size=BATCH_SIZE, augment=True) # chama função 3\n",
        "val_gen   = DDSMGenerator(val_df, batch_size=BATCH_SIZE, augment=False)\n",
        "\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        # Aumentei a paciência para 12 para dar mais tempo de adaptação ao fine-tuning\n",
        "        EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n",
        "        ModelCheckpoint('melhor_modelo_google_net.keras', save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        ")\n",
        "print(\"Treinamento finalizado.\")"
      ],
      "metadata": {
        "id": "PxuZk0wtc0_v",
        "outputId": "8572d9f4-3237-4252-e1aa-36468604489c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PxuZk0wtc0_v",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializando GoogleNet (InceptionV3)...\n",
            "Configurando Fine-Tuning (Descongelando topo da rede)...\n",
            "Compilando o modelo...\n",
            "Configurando geradores (Batch Size: 8)...\n",
            "\n",
            "Iniciando treinamento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 1s/step - accuracy: 0.5088 - auc: 0.5049 - loss: 0.7691 - val_accuracy: 0.5690 - val_auc: 0.5867 - val_loss: 0.6913\n",
            "Epoch 2/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1s/step - accuracy: 0.5733 - auc: 0.5919 - loss: 0.6888 - val_accuracy: 0.5711 - val_auc: 0.6005 - val_loss: 0.6818\n",
            "Epoch 3/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 2s/step - accuracy: 0.5533 - auc: 0.5672 - loss: 0.7176 - val_accuracy: 0.5625 - val_auc: 0.5999 - val_loss: 0.6914\n",
            "Epoch 4/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1s/step - accuracy: 0.5757 - auc: 0.5905 - loss: 0.6984 - val_accuracy: 0.5690 - val_auc: 0.6026 - val_loss: 0.6881\n",
            "Epoch 5/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - accuracy: 0.5673 - auc: 0.6321 - loss: 0.6638 - val_accuracy: 0.5431 - val_auc: 0.5895 - val_loss: 0.6970\n",
            "Epoch 6/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.5658 - auc: 0.6175 - loss: 0.6779 - val_accuracy: 0.5582 - val_auc: 0.6001 - val_loss: 0.6848\n",
            "Epoch 7/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.5776 - auc: 0.6412 - loss: 0.6594 - val_accuracy: 0.5388 - val_auc: 0.5819 - val_loss: 0.7059\n",
            "Epoch 8/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.5758 - auc: 0.6241 - loss: 0.6722 - val_accuracy: 0.5474 - val_auc: 0.5897 - val_loss: 0.7059\n",
            "Epoch 9/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 1s/step - accuracy: 0.5581 - auc: 0.6014 - loss: 0.6860 - val_accuracy: 0.5517 - val_auc: 0.5983 - val_loss: 0.6940\n",
            "Epoch 10/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.5951 - auc: 0.6555 - loss: 0.6571 - val_accuracy: 0.5582 - val_auc: 0.5969 - val_loss: 0.7002\n",
            "Epoch 11/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.6159 - auc: 0.6481 - loss: 0.6742 - val_accuracy: 0.5517 - val_auc: 0.5875 - val_loss: 0.7068\n",
            "Epoch 12/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.6240 - auc: 0.6803 - loss: 0.6337 - val_accuracy: 0.5517 - val_auc: 0.5850 - val_loss: 0.7037\n",
            "Epoch 13/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.6319 - auc: 0.6920 - loss: 0.6264 - val_accuracy: 0.5603 - val_auc: 0.5864 - val_loss: 0.7169\n",
            "Epoch 14/50\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - accuracy: 0.6049 - auc: 0.6596 - loss: 0.6552 - val_accuracy: 0.5431 - val_auc: 0.5806 - val_loss: 0.7341\n",
            "Treinamento finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outro**"
      ],
      "metadata": {
        "id": "AsXNR3VEhL7k"
      },
      "id": "AsXNR3VEhL7k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting (Sobreajuste)**\n",
        "\n",
        "A partir da época 12, a rede começou a \"decorar\" as imagens de treino (especialmente as repetidas da classe Normal) em vez de aprender os padrões gerais."
      ],
      "metadata": {
        "id": "4wTtPLPFhl3H"
      },
      "id": "4wTtPLPFhl3H"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. REDE NEURAL E TREINAMENTO (GOOGLE NET + ADAM) ---\n",
        "print(\"\\nInicializando GoogleNet (InceptionV3)...\")\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação: Adam com LR=0.0001\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Iniciando treinamento com dataset balanceado (1950 amostras)...\")\n",
        "\n",
        "# Treinamento\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=15, # Artigo usou até 100 iterações/épocas\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Treinamento concluído.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buR0MtVPhgOw",
        "outputId": "a05fae14-a906-42c2-f930-dd92fda80280"
      },
      "id": "buR0MtVPhgOw",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inicializando GoogleNet (InceptionV3)...\n",
            "Iniciando treinamento com dataset balanceado (1950 amostras)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.3897 - loss: 1.1359"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 6s/step - accuracy: 0.3900 - loss: 1.1355 - val_accuracy: 0.4241 - val_loss: 1.1640\n",
            "Epoch 2/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 6s/step - accuracy: 0.5174 - loss: 0.9867 - val_accuracy: 0.4983 - val_loss: 0.9821\n",
            "Epoch 3/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 6s/step - accuracy: 0.6026 - loss: 0.8236 - val_accuracy: 0.5362 - val_loss: 0.8799\n",
            "Epoch 4/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m818s\u001b[0m 6s/step - accuracy: 0.6039 - loss: 0.7907 - val_accuracy: 0.5310 - val_loss: 0.9732\n",
            "Epoch 5/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 6s/step - accuracy: 0.6274 - loss: 0.7505 - val_accuracy: 0.5655 - val_loss: 0.8934\n",
            "Epoch 6/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 6s/step - accuracy: 0.6833 - loss: 0.6681 - val_accuracy: 0.6172 - val_loss: 0.7900\n",
            "Epoch 7/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 6s/step - accuracy: 0.6916 - loss: 0.6317 - val_accuracy: 0.5707 - val_loss: 1.0297\n",
            "Epoch 8/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 6s/step - accuracy: 0.7516 - loss: 0.5610 - val_accuracy: 0.5983 - val_loss: 0.8453\n",
            "Epoch 9/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 6s/step - accuracy: 0.7449 - loss: 0.5591 - val_accuracy: 0.6138 - val_loss: 0.9781\n",
            "Epoch 10/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 6s/step - accuracy: 0.7628 - loss: 0.5053 - val_accuracy: 0.6500 - val_loss: 0.8385\n",
            "Epoch 11/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m809s\u001b[0m 6s/step - accuracy: 0.8187 - loss: 0.4580 - val_accuracy: 0.6017 - val_loss: 0.9155\n",
            "Epoch 12/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 6s/step - accuracy: 0.7812 - loss: 0.5116 - val_accuracy: 0.6655 - val_loss: 0.7451\n",
            "Epoch 13/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m855s\u001b[0m 6s/step - accuracy: 0.8475 - loss: 0.3751 - val_accuracy: 0.6328 - val_loss: 0.9668\n",
            "Epoch 14/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 6s/step - accuracy: 0.8756 - loss: 0.3286 - val_accuracy: 0.6034 - val_loss: 1.4516\n",
            "Epoch 15/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 6s/step - accuracy: 0.8843 - loss: 0.2965 - val_accuracy: 0.6224 - val_loss: 1.2298\n",
            "Treinamento concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rede aprendeu muito bem! Ela saiu de 36% para 95% no conjunto de treino. Isso prova que a arquitetura GoogleNet + Adam funciona e tem capacidade de aprender as características do câncer."
      ],
      "metadata": {
        "id": "SXW0LTz2iwgY"
      },
      "id": "SXW0LTz2iwgY"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "\n",
        "print(\"\\nInicializando GoogleNet (InceptionV3) para Fine-Tuning...\")\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 1. Descongelar as camadas do topo da GoogleNet\n",
        "# As primeiras camadas congeladas (traços básicos) e treinar as últimas (formas complexas)\n",
        "for layer in base_model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in base_model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# 2. Recompilar com Learning Rate MUITO baixo\n",
        "# Mantém Adam mas se reduz drasticamente o LR para não destruir o que já aprendeu.\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Iniciando Fine-Tuning (Descongelando camadas finais)...\")\n",
        "\n",
        "# 3. Continuar o treinamento\n",
        "history_fine = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=15,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulV3tNES-3_k",
        "outputId": "ac112907-79b6-440f-f656-0c47a07f9cc1"
      },
      "id": "ulV3tNES-3_k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inicializando GoogleNet (InceptionV3) para Fine-Tuning...\n",
            "Iniciando Fine-Tuning (Descongelando camadas finais)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 3s/step - accuracy: 0.3616 - loss: 1.1253 - val_accuracy: 0.4310 - val_loss: 1.0373\n",
            "Epoch 2/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 2s/step - accuracy: 0.5159 - loss: 1.0012 - val_accuracy: 0.5672 - val_loss: 0.9228\n",
            "Epoch 3/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - accuracy: 0.5833 - loss: 0.9005 - val_accuracy: 0.6310 - val_loss: 0.8410\n",
            "Epoch 4/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - accuracy: 0.6606 - loss: 0.7806 - val_accuracy: 0.6328 - val_loss: 0.7766\n",
            "Epoch 5/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 2s/step - accuracy: 0.6870 - loss: 0.7193 - val_accuracy: 0.6397 - val_loss: 0.7163\n",
            "Epoch 6/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 2s/step - accuracy: 0.7551 - loss: 0.5827 - val_accuracy: 0.6569 - val_loss: 0.6979\n",
            "Epoch 7/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 2s/step - accuracy: 0.7938 - loss: 0.5146 - val_accuracy: 0.6655 - val_loss: 0.6683\n",
            "Epoch 8/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 2s/step - accuracy: 0.8385 - loss: 0.4220 - val_accuracy: 0.6707 - val_loss: 0.6396\n",
            "Epoch 9/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 2s/step - accuracy: 0.8639 - loss: 0.3728 - val_accuracy: 0.6672 - val_loss: 0.6678\n",
            "Epoch 10/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - accuracy: 0.8764 - loss: 0.3513 - val_accuracy: 0.6552 - val_loss: 0.7043\n",
            "Epoch 11/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - accuracy: 0.9124 - loss: 0.2855 - val_accuracy: 0.6793 - val_loss: 0.7183\n",
            "Epoch 12/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - accuracy: 0.9108 - loss: 0.2648 - val_accuracy: 0.6586 - val_loss: 0.7800\n",
            "Epoch 13/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 2s/step - accuracy: 0.9209 - loss: 0.2246 - val_accuracy: 0.6466 - val_loss: 0.8652\n",
            "Epoch 14/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 2s/step - accuracy: 0.9456 - loss: 0.1844 - val_accuracy: 0.6431 - val_loss: 0.9116\n",
            "Epoch 15/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - accuracy: 0.9516 - loss: 0.1505 - val_accuracy: 0.6466 - val_loss: 0.9905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation\n",
        "\n",
        "Pós Geradores Prontos"
      ],
      "metadata": {
        "id": "EFFl7ob4m77z"
      },
      "id": "EFFl7ob4m77z"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# --- NOVO GERADOR COM DATA AUGMENTATION ---\n",
        "# Para combater o overfitting\n",
        "\n",
        "class AugmentedDDSMGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=10, augment=False):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.le = LabelEncoder()\n",
        "        self.df['enc'] = self.le.fit_transform(self.df['label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "\n",
        "        # Configuramos as distorções (rotação, zoom, flip)\n",
        "        if self.augment:\n",
        "            self.datagen = ImageDataGenerator(\n",
        "                rotation_range=20,      # Gira até 20 graus\n",
        "                width_shift_range=0.1,  # Move horizontalmente\n",
        "                height_shift_range=0.1, # Move verticalmente\n",
        "                shear_range=0.1,        # Inclina\n",
        "                zoom_range=0.1,         # Zoom leve\n",
        "                horizontal_flip=True,   # Espelha\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_data = self.df.iloc[indices]\n",
        "\n",
        "        X, y = [], []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            # 1. Processa a imagem (Wiener, CLAHE, etc)\n",
        "            img = process_pipeline(row['filepath'])\n",
        "            X.append(img)\n",
        "            y.append(row['enc'])\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = to_categorical(y, num_classes=3)\n",
        "\n",
        "        # 2. Se for treino, aplica a distorção aleatória na imagem processada\n",
        "        if self.augment:\n",
        "            # O flow do Keras espera 4D array, fazemos um truque para aplicar no batch\n",
        "            iter_batch = self.datagen.flow(X, y, batch_size=self.batch_size, shuffle=False)\n",
        "            return next(iter_batch)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# Recriar os geradores\n",
        "print(\"Recriando geradores com Data Augmentation...\")\n",
        "# Treino COM aumento de dados\n",
        "train_gen_aug = AugmentedDDSMGenerator(train_df, batch_size=10, augment=True)\n",
        "# Validação SEM aumento (queremos testar na imagem real)\n",
        "val_gen_aug = AugmentedDDSMGenerator(val_df, batch_size=10, augment=False)\n",
        "print(\"Pronto para re-treinar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhRYxBPwm6a7",
        "outputId": "395be918-d07a-4eeb-fac1-5a6e043a76f7"
      },
      "id": "UhRYxBPwm6a7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recriando geradores com Data Augmentation...\n",
            "Pronto para re-treinar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pós geradores com Data Augmentation"
      ],
      "metadata": {
        "id": "HTS2WmAEq__d"
      },
      "id": "HTS2WmAEq__d"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. REINICIALIZAÇÃO DO MODELO (Para limpar o Overfitting anterior) ---\n",
        "print(\"Reinicializando GoogleNet (InceptionV3) limpa...\")\n",
        "\n",
        "# Carrega a base com pesos da ImageNet (Transfer Learning)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Reconstrói as camadas finais conforme o artigo\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x) # Dropout de 50%\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilação exata do artigo: Adam com LR 0.0001\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Modelo pronto para receber dados com Augmentation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFjQHvSq_G5",
        "outputId": "e82e0c84-f6fb-4894-8799-d3f35e34c122"
      },
      "id": "4qFjQHvSq_G5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinicializando GoogleNet (InceptionV3) limpa...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Modelo pronto para receber dados com Augmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento Robusto (Com Data Augmentation)\n",
        "\n",
        "Estagnação/Underfitting (Treino ~55%, Validação ~52%). A rede está tendo dificuldade para aprender os padrões reais diante das variações (rotações, zoom)."
      ],
      "metadata": {
        "id": "6L_-dQqKrWyK"
      },
      "id": "6L_-dQqKrWyK"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. TREINAMENTO COM DATA AUGMENTATION ---\n",
        "print(\"Iniciando treinamento com Data Augmentation...\")\n",
        "\n",
        "history_aug = model.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug, # Validação estática\n",
        "    epochs=30,              # Aumentou-se as épocas\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Treinamento finalizado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmChPcoyrXOh",
        "outputId": "7b28d746-29ee-479e-c4aa-dea241aad132"
      },
      "id": "bmChPcoyrXOh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando treinamento com Data Augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4750 - loss: 1.0239"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 6s/step - accuracy: 0.4751 - loss: 1.0238 - val_accuracy: 0.5000 - val_loss: 0.9102\n",
            "Epoch 2/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 5s/step - accuracy: 0.4852 - loss: 0.9515 - val_accuracy: 0.4920 - val_loss: 0.9621\n",
            "Epoch 3/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 6s/step - accuracy: 0.5322 - loss: 0.8964 - val_accuracy: 0.4680 - val_loss: 0.9404\n",
            "Epoch 4/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 5s/step - accuracy: 0.5295 - loss: 0.9098 - val_accuracy: 0.5120 - val_loss: 0.8843\n",
            "Epoch 5/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 5s/step - accuracy: 0.5290 - loss: 0.9037 - val_accuracy: 0.5100 - val_loss: 0.8886\n",
            "Epoch 6/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 6s/step - accuracy: 0.5167 - loss: 0.8909 - val_accuracy: 0.5020 - val_loss: 1.3118\n",
            "Epoch 7/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.5351 - loss: 0.8943 - val_accuracy: 0.4960 - val_loss: 0.9134\n",
            "Epoch 8/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step - accuracy: 0.5196 - loss: 0.9243 - val_accuracy: 0.5160 - val_loss: 0.9499\n",
            "Epoch 9/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 6s/step - accuracy: 0.5388 - loss: 0.8462 - val_accuracy: 0.5140 - val_loss: 0.9128\n",
            "Epoch 10/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 6s/step - accuracy: 0.5273 - loss: 0.9055 - val_accuracy: 0.5280 - val_loss: 0.9127\n",
            "Epoch 11/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 6s/step - accuracy: 0.5608 - loss: 0.9044 - val_accuracy: 0.4940 - val_loss: 0.8878\n",
            "Epoch 12/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 6s/step - accuracy: 0.5196 - loss: 0.8729 - val_accuracy: 0.4840 - val_loss: 0.9432\n",
            "Epoch 13/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 6s/step - accuracy: 0.5185 - loss: 0.8570 - val_accuracy: 0.4980 - val_loss: 0.9221\n",
            "Epoch 14/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step - accuracy: 0.5491 - loss: 0.8600 - val_accuracy: 0.5240 - val_loss: 0.8757\n",
            "Epoch 15/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 6s/step - accuracy: 0.5204 - loss: 0.8722 - val_accuracy: 0.4800 - val_loss: 0.8927\n",
            "Epoch 16/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.5451 - loss: 0.9017 - val_accuracy: 0.5380 - val_loss: 0.9043\n",
            "Epoch 17/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 6s/step - accuracy: 0.5695 - loss: 0.8373 - val_accuracy: 0.5140 - val_loss: 0.9359\n",
            "Epoch 18/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 6s/step - accuracy: 0.5393 - loss: 0.8677 - val_accuracy: 0.5720 - val_loss: 0.9114\n",
            "Epoch 19/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 6s/step - accuracy: 0.5454 - loss: 0.8390 - val_accuracy: 0.5220 - val_loss: 0.8843\n",
            "Epoch 20/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 6s/step - accuracy: 0.5440 - loss: 0.8566 - val_accuracy: 0.5640 - val_loss: 0.9077\n",
            "Epoch 21/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 6s/step - accuracy: 0.5146 - loss: 0.8823 - val_accuracy: 0.5300 - val_loss: 0.8759\n",
            "Epoch 22/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 6s/step - accuracy: 0.5424 - loss: 0.8507 - val_accuracy: 0.5100 - val_loss: 0.8982\n",
            "Epoch 23/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 6s/step - accuracy: 0.5520 - loss: 0.8746 - val_accuracy: 0.5320 - val_loss: 0.9262\n",
            "Epoch 24/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.5627 - loss: 0.8564 - val_accuracy: 0.5540 - val_loss: 0.8663\n",
            "Epoch 25/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 6s/step - accuracy: 0.5468 - loss: 0.8534 - val_accuracy: 0.5260 - val_loss: 0.9669\n",
            "Epoch 26/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 6s/step - accuracy: 0.5485 - loss: 0.8645 - val_accuracy: 0.5120 - val_loss: 0.9835\n",
            "Epoch 27/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.5725 - loss: 0.8358 - val_accuracy: 0.5140 - val_loss: 0.9109\n",
            "Epoch 28/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 6s/step - accuracy: 0.5777 - loss: 0.8474 - val_accuracy: 0.5280 - val_loss: 0.8944\n",
            "Epoch 29/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 6s/step - accuracy: 0.5630 - loss: 0.8531 - val_accuracy: 0.5120 - val_loss: 0.8880\n",
            "Epoch 30/30\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 6s/step - accuracy: 0.5476 - loss: 0.8673 - val_accuracy: 0.5200 - val_loss: 0.9821\n",
            "Treinamento finalizado.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}